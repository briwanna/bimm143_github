---
title: "Class 8: Mini Project"
format: pdf
---

In todays mini project we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering nd PCA for now).

The data itself comes from the wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data.

#Save your input file into your Project directory

```{r}
fna.data <-  "WisconsinCancer (1).csv"
```

# Complete the following code to input the data and store as wics.df

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

Remove the diagnosis column and keep it in a separate vector for later.
# We can use -1 here to remove the first column

```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

# Explore the data analysis

The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.

>Q1. How many observations (patients) are in this dataset?

```{r}
nrow(wisc.data)
```


>Q2. How many of the observations have a malignant diagnosis?



```{r}
table(diagnosis)
```
>Q3. How many variables/features in the data are suffixed with _mean?

First find the column names
```{r}
colnames(wisc.data)
```

Next we need to search within the column for "_mean" pattern. The `grep()` function might  help here.

```{r}
inds <- grep("_mean", colnames(wisc.data))
length(inds)
```

> Q. How many dimensions are in this dataset?

```{r}
ncol(wisc.data)
```


# Principal Component Analysis

First do we need to scale the data before PCA or not.

```{r}
round(apply(wisc.data, 2, sd), 3)
```
Looks like we need to scale.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```


>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72%

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs capture 91%

## PC Plot

We need to make our plot 
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)

```

# Interpret PCA results

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is a difference in the distance but the plot needs to be more organized in order to understand the data better.
```{r}
biplot(wisc.pr)
```


>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
plot(wisc.pr$x[, 1],wisc.pr$x[, 3],  col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

##Scatter plot observations by components 1 and 2
```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis

ggplot(pc) +aes(PC1, PC2, col=diagnosis) + geom_point()
```


#Varience Explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.


```{r}
pve <- pr.var / sum(pr.var)
head(pve)
```

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# Examine the PC loadings

How much do the original variables contribute to the PCs that we have calculated? To get at this data we can look at the `$rotation` portion of the returned PCA object.

```{r}
head(wisc.pr$rotation[,1:3])
```
Focus in on PC1

```{r}
head(wisc.pr$rotation[,1])
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


There is a complicated mix of variables that go together to make up PC1 - 1.e. there are many of the original variables that together contribute highly tp PC1. 

```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings) +
  aes(PC1, rownames(loadings)) + 
  geom_col()
```


> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PCs to get 84.73%


#3. Hierarchical Clustering

The goal of this section is to do hierarchical clustering of the original data.

First we will scale the data

# Scale the wisc.data data using the "scale()" function


```{r}
data.scaled <- scale(wisc.data)
```

```{r}
wisc.hclust <- hclust(dist(scale(wisc.data)))
```


```{r}
plot(wisc.hclust)
```
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```


> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Cut this tree to yield cluster membership vector with `cutree()` function.

```{r}
grps <- cutree(wisc.hclust, h=19)
table(grps)
```

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=10)
table (grps, diagnosis)
```
The results from cutting the tree with k=10 allows us to get a better match of clusters vs. diagnosis with cluster 7 corresponding to benign cells.
```{r}
table(grps, diagnosis)
```
#Using different methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="single")
plot(wisc.pr.hclust)
```

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="average")
plot(wisc.pr.hclust)
```

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="complete")
plot(wisc.pr.hclust)
```
I like the "ward.D2" method the best because it allows for there to be minimum variance increase in the clusters. 
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```


#Combine Methods: PCA and HCLUST

My PCA results were interesting as they showed a separation of M and B samples along PC1.
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`.


Try clustering 3 PCs, that is PC1 PC2 and PC 3 
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```

And my tree results figure

```{r}
plot(wisc.pr.hclust)
```


Let's cut this tree into 4 groups/clusters

```{r}
grps <- cutree(wisc.pr.hclust, k=4)
table (grps)
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```
>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

How well do the two clusters separate the M and B diagnosis?
```{r}
table(grps, diagnosis)
```
The two clusters separate the two clusters well while four clusters is difficult to read. We cannot see the separation between the two diagnosis with four colors present. 

```{r}
(179+333)/nrow(wisc.data)
```







